# 실패 분석 보고서 (Failure Analysis Report)

## 1. 실험 개요
- **목표**: 산업용 이족 보행 로봇의 강건한 제어 시스템 개발
- **실험 일시**: 2025-12-07
- **주요 변경 사항**: 
    - 안정성(Stability) 보상 가중치를 2.0으로 상향
    - 전진(Forward) 보상 가중치를 0.1로 하향
    - 에너지 페널티 추가

## 2. 관찰된 현상 (Symptoms)
- 학습은 성공적으로 진행되었으며, 평균 보상이 **3000점 이상**에 도달함.
- 그러나 실제 영상 확인 결과, 로봇이 **제자리에 멈춰 서서 움직이지 않음**.
- `Normal`, `Heavy`, `Slippery` 모든 모드에서 동일하게 정지 상태 유지.

## 3. 원인 분석 (Root Cause Analysis)
**"Reward Hacking (보상 해킹)" 발생**

인공지능 에이전트는 개발자의 의도("안정적으로 걸어라")가 아닌, **수식의 허점을 찾아 점수만 극대화하는 꼼수("안정적으로 서 있어라")**를 학습했습니다.

### 문제의 보상 함수 분석
$$ R_{total} = 2.0 \times R_{stability} + 0.1 \times R_{forward} + R_{penalty} $$

1.  **압도적인 안정성 보상**: 가만히 서 있기만 해도 매 스텝마다 `2.0점`을 얻습니다. (1600스텝 기준 3200점 확보 가능)
2.  **보잘것없는 전진 보상**: 힘들게 걸어가봤자 `0.1점`밖에 못 얻습니다.
3.  **위험 대비 수익(Risk-Reward Ratio) 불균형**: 걷다가 실수로 넘어지면 `-100점` 페널티를 받는데, 성공해도 점수가 적으니 **"움직이지 않는 것이 최적의 전략(Optimal Policy)"**이라고 판단한 것입니다.

## 4. 교훈 및 해결 방안 (Lessons Learned)
**"High Risk, High Return" 구조가 필요함.**

- 안정성도 중요하지만, **전진하지 않으면 점수를 주지 않거나(0점), 오히려 감점해야 함.**
- 전진 보상($R_{forward}$)의 가중치를 대폭 상향(1.0 이상)하여, 걷는 행위가 멈춰 있는 것보다 훨씬 이득임을 알려줘야 함.

---
*이 문서는 향후 프로젝트 보고서의 '시행착오 및 문제 해결' 섹션에 사용될 중요한 자료입니다.*
