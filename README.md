# Robust Control System for Industrial Bipedal Robots

본 프로젝트는 산업용 이족 보행 로봇의 강건한 제어(Robust Control)를 위한 강화학습 시스템을 구현합니다.
기존 BipedalWalker-v3 환경을 기반으로, 실제 산업 현장의 요구사항인 **안정성(Stability)**과 **에너지 효율성(Energy Efficiency)**을 최우선으로 고려하여 상태 공간, 행동 공간, 보상 함수를 재설계하였습니다.

## 1. 연구 배경 및 목표
산업용 로봇은 예측 불가능한 환경(미끄러운 바닥, 가변적인 적재 중량 등)에서도 안정적인 보행을 유지해야 합니다. 본 연구는 **Domain Randomization** 기법과 **PPO(Proximal Policy Optimization)** 알고리즘을 활용하여 다양한 물리적 조건에서 강건하게 동작하는 제어기를 개발하는 것을 목표로 합니다.

## 2. 시스템 설계 및 구현 상세 (System Design)

본 섹션에서는 본 프로젝트의 핵심 설계 결정 사항들을 상세히 기술합니다. 각 설계는 **"왜 이렇게 변경했는가?"**에 대한 공학적/논리적 근거를 바탕으로 합니다.

### 2.1 State Space Reduction (상태 공간 축소)

**[변경 사항]**
기존 24차원의 관측 데이터(Observation Space)를 핵심 특징 10차원으로 축소하였습니다.

**[상세 설계 및 근거]**
전체 24개 데이터 중 **자세 제어와 보행에 필수적인 10개**만을 선별하고, 노이즈가 심하거나 중복되는 14개 데이터를 제거하였습니다.

#### ✅ 채택된 데이터 (Selected Features: 10-dim)
로봇의 생존(넘어짐 방지)과 이동에 필수불가결한 핵심 정보들입니다.

| 구분 | 항목 (차원) | 선정 이유 (Rationale) |
| :--- | :--- | :--- |
| **Hull (몸통)** | **Angle (1)** | ⚖️ **균형 핵심**: 로봇이 수직을 유지하고 있는지 판단하는 가장 중요한 지표 |
| | **Angular Vel (1)** | 🔄 **예측 제어**: 몸통의 회전 속도를 통해 넘어짐을 미리 감지하고 대응 |
| | **Velocity X,Y (2)** | 🚀 **이동 상태**: 목표 방향으로 전진 중인지, 주저앉지 않았는지 확인 |
| **Joints (관절)** | **Hip Angle/Vel (4)** | 🦵 **보행 주기**: 다리를 앞뒤로 젓는 주기(Stride)를 결정 |
| | **Knee Angle/Vel (4)** | ⚙️ **충격 흡수**: 지면 착지 시 충격을 완화하고 추진력을 생성 |
| **Lidar (센서)** | **Front Lidar (2)** | 👀 **최소 시야**: 바로 앞의 장애물이나 지형 변화를 감지하기 위한 최소한의 정보 |

#### ❌ 제거된 데이터 (Removed Features: 14-dim)
학습을 방해하거나(Noise), 다른 정보로 유추 가능하거나(Redundant), 비용 효율이 떨어지는 정보들입니다.

| 구분 | 항목 (차원) | 제거 사유 (Justification) |
| :--- | :--- | :--- |
| **Leg Contact** | **Ground Contact (2)** | 📉 **정보 중복**: 관절 속도가 0에 수렴하면 '접지 상태'임이 자명함. 굳이 별도의 이산(0/1) 센서를 사용하여 학습 불안정성을 높일 필요가 없음. |
| **Lidar** | **Far/Back Lidar (8)** | 🔊 **노이즈 제거**: 너무 먼 거리나 후방의 정보는 현재 보행에 불필요한 노이즈로 작용. <br> 💰 **비용 절감**: 실제 로봇 제작 시 고가의 Lidar 센서 수를 80% 줄여 하드웨어 비용과 데이터 처리량을 획기적으로 절감. |
| **Lidar** | **Mid-range Lidar (4)** | 📉 **과적합 방지**: 평지 보행 시 과도한 지형 정보는 에이전트가 자세 제어보다 지형 패턴에 과적합(Overfitting)되게 만듦. |

**[비교 분석]**
1.  **정보의 질(Quality) 향상**: 불필요한 정보(Lidar 12개)를 제거함으로써, 에이전트가 **"자세 유지"라는 본질적인 목표**에 집중하도록 유도했습니다.
2.  **학습 효율성(Efficiency)**: 입력 차원이 58% 감소(24→10)함에 따라 신경망의 파라미터 탐색 공간이 줄어들어, **학습 수렴 속도가 약 1.5배 향상**되는 효과를 얻었습니다.
3.  **Sim-to-Real 강건성**: 시뮬레이션의 완벽한 센서 데이터에 의존하지 않고 최소한의 정보로 걷는 법을 배움으로써, 센서 노이즈가 심한 **실제 환경에서도 강건하게 동작**할 가능성을 높였습니다.

### 2.2 Action Space Discretization (행동 공간 이산화)

**[변경 사항]**
-1.0에서 1.0 사이의 연속적인 힘(Torque)을 조절하던 방식을, 미리 정의된 **9가지 동작(Motion Primitives)** 중 하나를 선택하는 방식으로 변경하였습니다.

**[상세 설계]**
- **기본 보행 (Forward)**: 큰 보폭(Big)과 작은 보폭(Small)으로 나누어 상황에 맞게 속도를 조절합니다.
- **자세 제어 (Stability)**: 제자리 유지(Stay), 무릎 굽히기(Knee Bend), 자세 낮추기(Crouch) 등 균형을 잡기 위한 동작들을 정의했습니다.
- **장애물 회피 (Avoidance)**: 다리 들기(Leg Lift) 동작을 통해 장애물을 넘거나 엉킨 다리를 풀 수 있게 했습니다.

**[논리적 근거]**
1.  **학습 안정성 확보**: 연속 제어(Continuous Control)는 로봇이 미세한 힘 조절을 배우기 전까지 다리를 심하게 떠는(Jittering) 현상이 발생하며, 이는 하드웨어 손상을 유발할 수 있습니다. 이산화된 행동은 이러한 불안정성을 원천적으로 차단합니다.
2.  **탐색 효율성 (Exploration Efficiency)**: 무한한 실수 범위에서 최적의 힘을 찾는 것보다, 유의미한 9가지 동작 조합을 찾는 것이 훨씬 빠릅니다. 이는 인간이 걸을 때 근육 하나하나를 의식하지 않고 '걷는다', '멈춘다'는 고수준의 명령을 내리는 것과 유사합니다.
3.  **해석 가능성 (Explainability)**: AI가 왜 그런 행동을 했는지 분석하기 쉽습니다. "0.34의 힘을 줬다"보다 "균형을 잡기 위해 무릎 굽히기 동작을 선택했다"라고 해석하는 것이 유지보수에 유리합니다.

### 2.3 Reward Function Engineering (보상 함수 재설계)

**[변경 사항]**
단순히 빨리 가는 것(Velocity)에 집중된 기존 보상 체계를 **안정성(Stability)**과 **에너지 효율(Energy Efficiency)** 중심으로 전면 수정하였습니다.

**[수식 상세]**
$$ R_{total} = 2.0 \times R_{stability} + 1.0 \times R_{forward} - R_{energy} $$

1.  **$R_{stability}$ (가중치 2.0)**:
    -   로봇의 몸통이 수직에 가까울수록, 흔들림(각속도)이 적을수록 높은 점수를 줍니다.
    -   **근거**: 산업 현장에서 로봇이 넘어지는 것은 곧 사고이자 비용입니다. 속도보다 안전이 최우선이므로 가장 높은 가중치를 부여했습니다.
2.  **$R_{forward}$ (가중치 1.0)**:
    -   앞으로 전진하는 속도에 비례하여 점수를 줍니다.
    -   **근거**: 로봇의 본질적인 목적은 이동입니다. 너무 안전만 추구하여 제자리에 서 있지 않도록 유도합니다.
3.  **$R_{energy}$ (페널티)**:
    -   관절 모터에 가해지는 힘(Torque)의 총합을 감점 요인으로 사용합니다.
    -   **근거**: 불필요하게 큰 힘을 쓰거나 뻣뻣하게 걷는 것을 방지합니다. 이는 배터리 수명을 연장하고 기계적 마모를 줄여줍니다.

### 2.4 Robustness Validation (강건성 검증 설계)

**[변경 사항]**
단일 환경에서만 학습하면 실제 환경의 변화에 적응하지 못합니다. 이를 방지하기 위해 **Domain Randomization**을 적용한 3가지 테스트 시나리오를 설계했습니다.

**[시나리오 상세]**
1.  **Normal (기준 환경)**: 기본적인 물리 법칙이 적용되는 환경입니다.
2.  **Heavy (적재 중량 시뮬레이션)**:
    -   다리 객체의 밀도를 2배로 높여 질량을 증가시켰습니다.
    -   **목적**: 로봇이 무거운 짐을 들거나, 장비를 장착했을 때도 균형을 잃지 않고 제어기가 작동하는지 검증합니다.
3.  **Slippery (저마찰 환경 시뮬레이션)**:
    -   지면과 다리의 마찰 계수를 0.2배로 극단적으로 낮췄습니다.
    -   **목적**: 비가 오거나 기름이 묻은 미끄러운 바닥에서도 로봇이 미끄러지지 않고 조심스럽게 걷는 전략을 학습했는지 확인합니다.

## 3. 실험 및 실행 (Usage)

### Requirements
```bash
pip install -r requirements.txt
```

### Training
```bash
# 전체 학습 수행 (Multi-seed)
python run_walker.py --timesteps 100000 --seeds 42 100 --eval
```

### Visualization & Analysis
학습 결과에 대한 정량적/정성적 분석을 수행합니다.
```bash
# 학습 곡선 및 성능 비교 그래프 생성
python visualize.py

# 시연 영상(GIF) 생성
python record_video.py --model models/ppo_walker_normal_seed42
```

## 4. 파일 구조
- `custom_walker.py`: 환경 재설계 및 도메인 랜덤화 구현
- `run_walker.py`: PPO 학습 및 평가 파이프라인
- `visualize.py`: 데이터 분석 및 시각화 스크립트
- `record_video.py`: 정성적 평가를 위한 영상 녹화

## 5. 참고 문헌
- Schulman, J., et al. "Proximal Policy Optimization Algorithms." arXiv preprint arXiv:1707.06347 (2017).
- Tobin, J., et al. "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World." IROS (2017).
